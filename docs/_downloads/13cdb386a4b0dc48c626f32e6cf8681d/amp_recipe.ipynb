{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\n**\u4f5c\u8005**: [Michael Carilli](https://github.com/mcarilli)\n\n[torch.cuda.amp](https://pytorch.org/docs/stable/amp.html) \u63d0\u4f9b\u4e86\u6df7\u5408\u7cbe\u5ea6\u7684\u4fbf\u5229\u65b9\u6cd5,\n\u5176\u4e2d\u4e00\u4e9b\u64cd\u4f5c\u4f7f\u7528 ``torch.float32`` (``float``) \u6570\u636e\u7c7b\u578b,\u800c\u53e6\u4e00\u4e9b\u64cd\u4f5c\u4f7f\u7528 ``torch.float16`` (``half``)\u3002\n\u4e00\u4e9b\u64cd\u4f5c,\u5982\u7ebf\u6027\u5c42\u548c\u5377\u79ef,\u5728 ``float16`` \u6216 ``bfloat16`` \u4e0b\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u3002\n\u800c\u5176\u4ed6\u64cd\u4f5c,\u5982\u5f52\u7ea6\u64cd\u4f5c,\u901a\u5e38\u9700\u8981 ``float32`` \u7684\u52a8\u6001\u8303\u56f4\u3002\u6df7\u5408\u7cbe\u5ea6\u8bd5\u56fe\u5c06\u6bcf\u4e2a\u64cd\u4f5c\u4e0e\u5176\u5408\u9002\u7684\u6570\u636e\u7c7b\u578b\u76f8\u5339\u914d,\n\u4ece\u800c\u51cf\u5c11\u7f51\u7edc\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528\u3002\n\n\u901a\u5e38,\"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\"\u540c\u65f6\u4f7f\u7528 [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast) \u548c\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)\u3002\n\n\u672c\u6559\u7a0b\u6d4b\u91cf\u4e86\u4e00\u4e2a\u7b80\u5355\u7f51\u7edc\u5728\u9ed8\u8ba4\u7cbe\u5ea6\u4e0b\u7684\u6027\u80fd,\u7136\u540e\u901a\u8fc7\u6dfb\u52a0 ``autocast`` \u548c ``GradScaler`` \u4ee5\u6df7\u5408\u7cbe\u5ea6\u8fd0\u884c\u76f8\u540c\u7684\u7f51\u7edc,\u63d0\u9ad8\u6027\u80fd\u3002\n\n\u60a8\u53ef\u4ee5\u4e0b\u8f7d\u5e76\u8fd0\u884c\u672c\u6559\u7a0b\u4f5c\u4e3a\u72ec\u7acb\u7684 Python \u811a\u672c\u3002\u552f\u4e00\u7684\u8981\u6c42\u662f PyTorch 1.6 \u6216\u66f4\u9ad8\u7248\u672c,\u4ee5\u53ca\u652f\u6301 CUDA \u7684 GPU\u3002\n\n\u6df7\u5408\u7cbe\u5ea6\u4e3b\u8981\u53d7\u76ca\u4e8e\u652f\u6301\u5f20\u91cf\u6838\u5fc3\u7684\u67b6\u6784(Volta\u3001Turing\u3001Ampere)\u3002\u5728\u8fd9\u4e9b\u67b6\u6784\u4e0a,\u672c\u6559\u7a0b\u5e94\u663e\u793a\u663e\u8457\u7684(2-3\u500d)\u52a0\u901f\u3002\n\u5728\u8f83\u65e9\u7684\u67b6\u6784(Kepler\u3001Maxwell\u3001Pascal)\u4e0a,\u60a8\u53ef\u80fd\u4f1a\u89c2\u5bdf\u5230\u9002\u5ea6\u7684\u52a0\u901f\u3002\n\u8fd0\u884c ``nvidia-smi`` \u53ef\u4ee5\u663e\u793a\u60a8\u7684 GPU \u67b6\u6784\u3002\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n\n# \u8ba1\u65f6\u5de5\u5177\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\n\u4ee5\u4e0b\u7ebf\u6027\u5c42\u548c ReLU \u7684\u5e8f\u5217\u5e94\u8be5\u5728\u6df7\u5408\u7cbe\u5ea6\u4e0b\u663e\u793a\u52a0\u901f\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append(torch.nn.Linear(in_size, in_size))\n        layers.append(torch.nn.ReLU())\n    layers.append(torch.nn.Linear(in_size, out_size))\n    return torch.nn.Sequential(*tuple(layers)).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``batch_size``\u3001``in_size``\u3001``out_size`` \u548c ``num_layers`` \u88ab\u9009\u62e9\u4e3a\u8db3\u591f\u5927\u7684\u503c,\u4ee5\u9971\u548c GPU \u5de5\u4f5c\u8d1f\u8f7d\u3002\n\u901a\u5e38,\u5f53 GPU \u9971\u548c\u65f6,\u6df7\u5408\u7cbe\u5ea6\u63d0\u4f9b\u7684\u52a0\u901f\u6700\u5927\u3002\n\u5c0f\u578b\u7f51\u7edc\u53ef\u80fd\u53d7 CPU \u9650\u5236,\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u6df7\u5408\u7cbe\u5ea6\u4e0d\u4f1a\u63d0\u9ad8\u6027\u80fd\u3002\n\u8fd9\u4e9b\u5927\u5c0f\u8fd8\u88ab\u9009\u62e9\u4e3a\u7ebf\u6027\u5c42\u7684\u53c2\u4e0e\u7ef4\u5ea6\u662f 8 \u7684\u500d\u6570,\u4ee5\u5141\u8bb8\u5728\u652f\u6301\u5f20\u91cf\u6838\u5fc3\u7684 GPU \u4e0a\u4f7f\u7528\u5f20\u91cf\u6838\u5fc3(\u89c1\u4e0b\u9762\u7684 `\u6545\u969c\u6392\u9664<troubleshooting>`)\u3002\n\n\u7ec3\u4e60:\u6539\u53d8\u53c2\u4e0e\u5927\u5c0f,\u89c2\u5bdf\u6df7\u5408\u7cbe\u5ea6\u52a0\u901f\u7684\u53d8\u5316\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 512 # \u5c1d\u8bd5,\u4f8b\u5982 128\u3001256\u3001513\u3002\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)\n\n# \u4ee5\u9ed8\u8ba4\u7cbe\u5ea6\u521b\u5efa\u6570\u636e\u3002\n# \u4e0b\u9762\u7684\u9ed8\u8ba4\u7cbe\u5ea6\u548c\u6df7\u5408\u7cbe\u5ea6\u8bd5\u9a8c\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u3002\n# \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u65f6,\u60a8\u4e0d\u9700\u8981\u624b\u52a8\u66f4\u6539\u8f93\u5165\u7684 ``dtype``\u3002\ndata = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\ntargets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n\nloss_fn = torch.nn.MSELoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u9ed8\u8ba4\u7cbe\u5ea6\n\u4e0d\u4f7f\u7528 ``torch.cuda.amp`` \u65f6,\u4ee5\u4e0b\u7b80\u5355\u7f51\u7edc\u4ee5\u9ed8\u8ba4\u7cbe\u5ea6( ``torch.float32`` )\u6267\u884c\u6240\u6709\u64cd\u4f5c:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True \u8fd9\u91cc\u53ef\u4ee5\u9002\u5ea6\u63d0\u9ad8\u6027\u80fd\nend_timer_and_print(\"Default precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u6dfb\u52a0 ``torch.autocast``\n[torch.autocast](https://pytorch.org/docs/stable/amp.html#autocasting) \u7684\u5b9e\u4f8b\n\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668,\u5141\u8bb8\u811a\u672c\u7684\u67d0\u4e9b\u533a\u57df\u4ee5\u6df7\u5408\u7cbe\u5ea6\u8fd0\u884c\u3002\n\n\u5728\u8fd9\u4e9b\u533a\u57df\u4e2d,CUDA \u64cd\u4f5c\u4ee5 ``autocast`` \u9009\u62e9\u7684 ``dtype`` \u8fd0\u884c,\n\u4ee5\u63d0\u9ad8\u6027\u80fd,\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002\n\u6709\u5173 ``autocast`` \u4e3a\u6bcf\u4e2a\u64cd\u4f5c\u9009\u62e9\u7684\u7cbe\u5ea6\u4ee5\u53ca\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u9009\u62e9\u7684\u8be6\u7ec6\u4fe1\u606f,\u8bf7\u53c2\u9605\n[Autocast \u64cd\u4f5c\u53c2\u8003](https://pytorch.org/docs/stable/amp.html#autocast-op-reference)\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 \u4e2a epoch,\u6b64\u90e8\u5206\u4ec5\u7528\u4e8e\u8bf4\u660e\n    for input, target in zip(data, targets):\n        # \u5728 ``autocast`` \u4e0b\u8fd0\u884c\u524d\u5411\u4f20\u9012\u3002\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            # \u8f93\u51fa\u662f float16,\u56e0\u4e3a\u7ebf\u6027\u5c42 ``autocast`` \u5230 float16\u3002\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # \u635f\u5931\u662f float32,\u56e0\u4e3a ``mse_loss`` \u5c42 ``autocast`` \u5230 float32\u3002\n            assert loss.dtype is torch.float32\n\n        # \u5728 backward() \u4e4b\u524d\u9000\u51fa ``autocast``\u3002\n        # \u4e0d\u5efa\u8bae\u5728 ``autocast`` \u4e0b\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\n        # \u53cd\u5411\u64cd\u4f5c\u4ee5 ``autocast`` \u4e3a\u76f8\u5e94\u524d\u5411\u64cd\u4f5c\u9009\u62e9\u7684\u76f8\u540c ``dtype`` \u8fd0\u884c\u3002\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True \u8fd9\u91cc\u53ef\u4ee5\u9002\u5ea6\u63d0\u9ad8\u6027\u80fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u6dfb\u52a0 ``GradScaler``\n[\u68af\u5ea6\u7f29\u653e](https://pytorch.org/docs/stable/amp.html#gradient-scaling)\n\u6709\u52a9\u4e8e\u9632\u6b62\u68af\u5ea6\u5e45\u5ea6\u8f83\u5c0f\u65f6\u5728\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u88ab\u51b2\u5237\u4e3a\u96f6\n(\"\u4e0b\u6ea2\")\u3002\n\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)\n\u65b9\u4fbf\u5730\u6267\u884c\u68af\u5ea6\u7f29\u653e\u7684\u6b65\u9aa4\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u5728\u6536\u655b\u8fd0\u884c\u5f00\u59cb\u65f6\u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570\u6784\u9020\u4e00\u4e2a ``scaler``\u3002\n# \u5982\u679c\u60a8\u7684\u7f51\u7edc\u5728\u9ed8\u8ba4 ``GradScaler`` \u53c2\u6570\u4e0b\u65e0\u6cd5\u6536\u655b,\u8bf7\u63d0\u4ea4\u4e00\u4e2a issue\u3002\n# \u6574\u4e2a\u6536\u655b\u8fd0\u884c\u5e94\u8be5\u4f7f\u7528\u76f8\u540c\u7684 ``GradScaler`` \u5b9e\u4f8b\u3002\n# \u5982\u679c\u60a8\u5728\u540c\u4e00\u4e2a\u811a\u672c\u4e2d\u6267\u884c\u591a\u4e2a\u6536\u655b\u8fd0\u884c,\u6bcf\u4e2a\u8fd0\u884c\u5e94\u8be5\u4f7f\u7528\u4e00\u4e2a\u4e13\u7528\u7684\u65b0 ``GradScaler`` \u5b9e\u4f8b\u3002``GradScaler`` \u5b9e\u4f8b\u662f\u8f7b\u91cf\u7ea7\u7684\u3002\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(0): # 0 \u4e2a epoch,\u6b64\u90e8\u5206\u4ec5\u7528\u4e8e\u8bf4\u660e\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # \u7f29\u653e\u635f\u5931\u3002\u5728\u7f29\u653e\u540e\u7684\u635f\u5931\u4e0a\u8c03\u7528 ``backward()`` \u4ee5\u521b\u5efa\u7f29\u653e\u540e\u7684\u68af\u5ea6\u3002\n        scaler.scale(loss).backward()\n\n        # ``scaler.step()`` \u9996\u5148\u5c06\u4f18\u5316\u5668\u5206\u914d\u7684\u53c2\u6570\u7684\u68af\u5ea6\u53cd\u7f29\u653e\u3002\n        # \u5982\u679c\u8fd9\u4e9b\u68af\u5ea6\u4e0d\u5305\u542b ``inf`` \u6216 ``NaN``s,\u5219\u8c03\u7528 optimizer.step(),\n        # \u5426\u5219\u8df3\u8fc7 optimizer.step()\u3002\n        scaler.step(opt)\n\n        # \u66f4\u65b0\u4e0b\u4e00\u6b21\u8fed\u4ee3\u7684\u7f29\u653e\u6bd4\u4f8b\u3002\n        scaler.update()\n\n        opt.zero_grad() # set_to_none=True \u8fd9\u91cc\u53ef\u4ee5\u9002\u5ea6\u63d0\u9ad8\u6027\u80fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u5168\u90e8\u96c6\u6210: \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\n(\u4ee5\u4e0b\u8fd8\u6f14\u793a\u4e86 ``enabled`` \u53c2\u6570,\u8fd9\u662f ``autocast`` \u548c ``GradScaler`` \u7684\u4e00\u4e2a\u53ef\u9009\u4fbf\u5229\u53c2\u6570\u3002\n\u5982\u679c\u4e3a False, ``autocast`` \u548c ``GradScaler`` \u7684\u8c03\u7528\u5c06\u6210\u4e3a\u65e0\u64cd\u4f5c\u3002\n\u8fd9\u5141\u8bb8\u5728\u9ed8\u8ba4\u7cbe\u5ea6\u548c\u6df7\u5408\u7cbe\u5ea6\u4e4b\u95f4\u5207\u6362,\u800c\u65e0\u9700\u4f7f\u7528 if/else \u8bed\u53e5\u3002)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True \u8fd9\u91cc\u53ef\u4ee5\u9002\u5ea6\u63d0\u9ad8\u6027\u80fd\nend_timer_and_print(\"\u6df7\u5408\u7cbe\u5ea6:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u68c0\u67e5/\u4fee\u6539\u68af\u5ea6(\u4f8b\u5982,\u68af\u5ea6\u88c1\u526a)\n``scaler.scale(loss).backward()`` \u4ea7\u751f\u7684\u6240\u6709\u68af\u5ea6\u90fd\u662f\u7f29\u653e\u8fc7\u7684\u3002\n\u5982\u679c\u60a8\u5e0c\u671b\u5728 ``backward()`` \u548c ``scaler.step(optimizer)`` \u4e4b\u95f4\u68c0\u67e5\u6216\u4fee\u6539\n\u53c2\u6570\u7684 ``.grad`` \u5c5e\u6027,\u60a8\u5e94\u8be5\u9996\u5148\u4f7f\u7528 \n[scaler.unscale_(optimizer)](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_) \u5bf9\u5b83\u4eec\u8fdb\u884c\u53cd\u7f29\u653e\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 0\u4e2aepoch,\u8fd9\u4e00\u90e8\u5206\u4ec5\u7528\u4e8e\u8bf4\u660e\nfor epoch in range(0):  \n    for input, target in zip(data, targets):\n        # \u5728 ``autocast`` \u4e0b\u8fd0\u884c\u524d\u5411\u4f20\u64ad\u3002\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            # output \u662f float16 \u56e0\u4e3a\u7ebf\u6027\u5c42\u4f1a ``autocast`` \u5230 float16\u3002\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss \u662f float32 \u56e0\u4e3a ``mse_loss`` \u5c42\u4f1a ``autocast`` \u5230 float32\u3002\n            assert loss.dtype is torch.float32\n\n        # \u5728 backward() \u4e4b\u524d\u9000\u51fa ``autocast``\u3002\n        # \u4e0d\u63a8\u8350\u5728 ``autocast`` \u4e0b\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\n        # \u53cd\u5411\u4f20\u64ad\u7684 ops \u5728\u4e0e\u5bf9\u5e94\u524d\u5411\u4f20\u64ad\u76f8\u540c\u7684 ``dtype`` \u4e0b\u8fd0\u884c\u3002\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True \u8fd9\u91cc\u53ef\u4ee5\u7565\u5fae\u63d0\u9ad8\u6027\u80fd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u4fdd\u5b58/\u6062\u590d\n\u8981\u4ee5\u4f4d\u7ea7\u7cbe\u5ea6\u4fdd\u5b58/\u6062\u590d\u542f\u7528\u4e86 Amp \u7684\u8fd0\u884c,\u8bf7\u4f7f\u7528\n[scaler.state_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict) \u548c\n[scaler.load_state_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict)\u3002\n\n\u4fdd\u5b58\u65f6,\u5c06 ``scaler`` \u7684\u72b6\u6001\u5b57\u5178\u4e0e\u901a\u5e38\u7684\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\u4e00\u8d77\u4fdd\u5b58\u3002\n\u53ef\u4ee5\u5728\u8fed\u4ee3\u5f00\u59cb\u65f6,\u4efb\u4f55\u524d\u5411\u4f20\u64ad\u4e4b\u524d,\u6216\u5728\u8fed\u4ee3\u7ed3\u675f\u65f6,\u5728 ``scaler.update()`` \u4e4b\u540e\u6267\u884c\u6b64\u64cd\u4f5c\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# \u6309\u9700\u5199\u5165\u68c0\u67e5\u70b9,\u4f8b\u5982:\n# torch.save(checkpoint, \"filename\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6062\u590d\u65f6,\u5c06 ``scaler`` \u7684\u72b6\u6001\u5b57\u5178\u4e0e\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\u4e00\u8d77\u52a0\u8f7d\u3002\n\u6309\u9700\u8bfb\u53d6\u68c0\u67e5\u70b9,\u4f8b\u5982:\n\n```\ndev = torch.cuda.current_device()\ncheckpoint = torch.load(\"filename\",\n                        map_location = lambda storage, loc: storage.cuda(dev))\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5982\u679c\u68c0\u67e5\u70b9\u662f\u4ece\u4e00\u4e2a\u6ca1\u6709\u4f7f\u7528 Amp \u7684\u8fd0\u884c\u4e2d\u521b\u5efa\u7684,\u800c\u60a8\u60f3\u6062\u590d\u8bad\u7ec3\u65f6\u4f7f\u7528 Amp,\n\u50cf\u5f80\u5e38\u4e00\u6837\u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\u3002\u68c0\u67e5\u70b9\u4e0d\u4f1a\u5305\u542b\u5df2\u4fdd\u5b58\u7684 ``scaler`` \u72b6\u6001,\u56e0\u6b64\n\u4f7f\u7528\u4e00\u4e2a\u65b0\u7684 ``GradScaler`` \u5b9e\u4f8b\u3002\n\n\u5982\u679c\u68c0\u67e5\u70b9\u662f\u4ece\u4e00\u4e2a\u4f7f\u7528\u4e86 Amp \u7684\u8fd0\u884c\u4e2d\u521b\u5efa\u7684,\u800c\u60a8\u60f3\u6062\u590d\u8bad\u7ec3\u65f6\u4e0d\u4f7f\u7528 ``Amp``,\n\u50cf\u5f80\u5e38\u4e00\u6837\u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001,\u5e76\u5ffd\u7565\u5df2\u4fdd\u5b58\u7684 ``scaler`` \u72b6\u6001\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u63a8\u7406/\u8bc4\u4f30\n``autocast`` \u53ef\u4ee5\u5355\u72ec\u7528\u4e8e\u5305\u88c5\u63a8\u7406\u6216\u8bc4\u4f30\u7684\u524d\u5411\u4f20\u64ad\u3002\u4e0d\u9700\u8981 ``GradScaler``\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## \u9ad8\u7ea7\u4e3b\u9898\n\u8bf7\u53c2\u9605 [\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u793a\u4f8b](https://pytorch.org/docs/stable/notes/amp_examples.html) \u4ee5\u4e86\u89e3\u9ad8\u7ea7\u7528\u4f8b,\u5305\u62ec:\n\n* \u68af\u5ea6\u7d2f\u79ef\n* \u68af\u5ea6\u60e9\u7f5a/\u53cc\u5411\u53cd\u5411\u4f20\u64ad\n* \u5305\u542b\u591a\u4e2a\u6a21\u578b\u3001\u4f18\u5316\u5668\u6216\u635f\u5931\u7684\u7f51\u7edc\n* \u591a GPU (``torch.nn.DataParallel`` \u6216 ``torch.nn.parallel.DistributedDataParallel``)\n* \u81ea\u5b9a\u4e49\u81ea\u52a8\u68af\u5ea6\u51fd\u6570 (``torch.autograd.Function`` \u7684\u5b50\u7c7b)\n\n\u5982\u679c\u5728\u540c\u4e00\u4e2a\u811a\u672c\u4e2d\u6267\u884c\u591a\u4e2a\u6536\u655b\u8fd0\u884c,\u6bcf\u4e2a\u8fd0\u884c\u90fd\u5e94\u8be5\u4f7f\u7528\u4e00\u4e2a\u4e13\u7528\u7684\u65b0 ``GradScaler`` \u5b9e\u4f8b\u3002``GradScaler`` \u5b9e\u4f8b\u662f\u8f7b\u91cf\u7ea7\u7684\u3002\n\n\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u8c03\u5ea6\u7a0b\u5e8f\u6ce8\u518c\u81ea\u5b9a\u4e49 C++ op,\u8bf7\u53c2\u9605\n[\u8c03\u5ea6\u7a0b\u5e8f\u6559\u7a0b](https://pytorch.org/tutorials/advanced/dispatcher.html#autocast) \u4e2d\u7684 `autocast \u90e8\u5206`\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## \u6545\u969c\u6392\u9664\n\u4f7f\u7528 Amp \u7684\u52a0\u901f\u6548\u679c\u5fae\u4e4e\u5176\u5fae\n~~~~~~~~~~~~~~~~~~~~~~~~~\n1. \u60a8\u7684\u7f51\u7edc\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5229\u7528 GPU \u7684\u8ba1\u7b97\u80fd\u529b,\u56e0\u6b64\u53d7\u5230 CPU \u7684\u9650\u5236\u3002Amp \u5bf9 GPU \u6027\u80fd\u7684\u5f71\u54cd\u5c06\u65e0\u5173\u7d27\u8981\u3002\n\n   * \u4e00\u4e2a\u7c97\u7565\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f,\u5c3d\u53ef\u80fd\u589e\u52a0\u6279\u91cf\u548c/\u6216\u7f51\u7edc\u5927\u5c0f,\u76f4\u5230\u4e0d\u4f1a\u53d1\u751f\u5185\u5b58\u4e0d\u8db3\u9519\u8bef\u3002\n   * \u5c3d\u91cf\u907f\u514d\u8fc7\u591a\u7684 CPU-GPU \u540c\u6b65 (``.item()`` \u8c03\u7528\u6216\u4ece CUDA \u5f20\u91cf\u6253\u5370\u503c)\u3002\n   * \u5c3d\u91cf\u907f\u514d\u5927\u91cf\u5c0f\u578b CUDA \u64cd\u4f5c\u7684\u5e8f\u5217 (\u5982\u679c\u53ef\u80fd,\u8bf7\u5c06\u8fd9\u4e9b\u64cd\u4f5c\u5408\u5e76\u4e3a\u51e0\u4e2a\u5927\u578b CUDA \u64cd\u4f5c)\u3002\n2. \u60a8\u7684\u7f51\u7edc\u53ef\u80fd\u662f GPU \u8ba1\u7b97\u5bc6\u96c6\u578b\u7684 (\u5927\u91cf ``matmuls``/\u5377\u79ef),\u4f46\u60a8\u7684 GPU \u6ca1\u6709\u5f20\u91cf\u6838\u5fc3\u3002\n   \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u9884\u671f\u52a0\u901f\u6548\u679c\u4f1a\u964d\u4f4e\u3002\n3. ``matmul`` \u7684\u7ef4\u5ea6\u4e0d\u9002\u5408\u5f20\u91cf\u6838\u5fc3\u3002\u8bf7\u786e\u4fdd\u53c2\u4e0e\u8ba1\u7b97\u7684 ``matmuls`` \u7684\u5927\u5c0f\u662f 8 \u7684\u500d\u6570\u3002\n   (\u5bf9\u4e8e\u5e26\u6709 encoders/decoders \u7684 NLP \u6a21\u578b,\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u5fae\u5999\u7684\u95ee\u9898\u3002\u6b64\u5916,\u65e9\u671f\u7248\u672c\u7684\u5377\u79ef\u4e5f\u6709\u7c7b\u4f3c\u7684\u5c3a\u5bf8\u9650\u5236,\u4ee5\u4fbf\u4f7f\u7528\u5f20\u91cf\u6838\u5fc3,\n   \u4f46\u5bf9\u4e8e CuDNN 7.3 \u53ca\u66f4\u9ad8\u7248\u672c,\u4e0d\u5b58\u5728\u6b64\u7c7b\u9650\u5236\u3002\u8bf7\u53c2\u9605 [\u8fd9\u91cc](https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841) \u4ee5\u83b7\u53d6\u6307\u5bfc\u3002)\n\n### \u635f\u5931\u662f inf/NaN\n\u9996\u5148,\u68c0\u67e5\u60a8\u7684\u7f51\u7edc\u662f\u5426\u7b26\u5408 `\u9ad8\u7ea7\u7528\u4f8b<advanced-topics>`\u3002\n\u53e6\u8bf7\u53c2\u9605 [\u4f18\u5148\u4f7f\u7528 binary_cross_entropy_with_logits \u800c\u4e0d\u662f binary_cross_entropy](https://pytorch.org/docs/stable/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy)\u3002\n\n\u5982\u679c\u60a8\u786e\u4fe1\u60a8\u7684 Amp \u7528\u6cd5\u662f\u6b63\u786e\u7684,\u60a8\u53ef\u80fd\u9700\u8981\u63d0\u4ea4\u4e00\u4e2a issue,\u4f46\u5728\u8fd9\u6837\u505a\u4e4b\u524d,\u6536\u96c6\u4ee5\u4e0b\u4fe1\u606f\u4f1a\u5f88\u6709\u5e2e\u52a9:\n\n1. \u901a\u8fc7\u5c06 ``enabled=False`` \u4f20\u9012\u7ed9\u5b83\u4eec\u7684\u6784\u9020\u51fd\u6570,\u5206\u522b\u7981\u7528 ``autocast`` \u6216 ``GradScaler``,\u5e76\u67e5\u770b ``infs``/``NaNs`` \u662f\u5426\u4ecd\u7136\u5b58\u5728\u3002\n2. \u5982\u679c\u60a8\u6000\u7591\u7f51\u7edc\u7684\u67d0\u4e00\u90e8\u5206 (\u4f8b\u5982,\u4e00\u4e2a\u590d\u6742\u7684\u635f\u5931\u51fd\u6570) \u6ea2\u51fa,\u8bf7\u5728 ``float32`` \u4e2d\u8fd0\u884c\u8be5\u524d\u5411\u533a\u57df,\n   \u5e76\u67e5\u770b ``infs``/``NaN``s \u662f\u5426\u4ecd\u7136\u5b58\u5728\u3002\n   [autocast \u6587\u6863\u5b57\u7b26\u4e32](https://pytorch.org/docs/stable/amp.html#torch.autocast) \u7684\u6700\u540e\u4e00\u4e2a\u4ee3\u7801\u7247\u6bb5\n   \u5c55\u793a\u4e86\u5982\u4f55\u5f3a\u5236\u5b50\u533a\u57df\u5728 ``float32`` \u4e2d\u8fd0\u884c (\u901a\u8fc7\u5728\u672c\u5730\u7981\u7528 ``autocast`` \u5e76\u5c06\u5b50\u533a\u57df\u7684\u8f93\u5165\u8f6c\u6362\u4e3a ``float32``)\u3002\n\n### \u7c7b\u578b\u4e0d\u5339\u914d\u9519\u8bef (\u53ef\u80fd\u8868\u73b0\u4e3a ``CUDNN_STATUS_BAD_PARAM``)\n``Autocast`` \u8bd5\u56fe\u6db5\u76d6\u6240\u6709\u53ef\u4ece\u4e2d\u53d7\u76ca\u6216\u9700\u8981\u8f6c\u6362\u7684 ops\u3002\n[\u83b7\u5f97\u660e\u786e\u8986\u76d6\u7684 ops](https://pytorch.org/docs/stable/amp.html#autocast-op-reference)\n\u662f\u6839\u636e\u6570\u503c\u5c5e\u6027\u9009\u62e9\u7684,\u4f46\u4e5f\u57fa\u4e8e\u7ecf\u9a8c\u3002\n\u5982\u679c\u60a8\u5728\u542f\u7528\u4e86 ``autocast`` \u7684\u524d\u5411\u533a\u57df\u6216\u968f\u540e\u7684\u53cd\u5411\u4f20\u64ad\u4e2d\u770b\u5230\u7c7b\u578b\u4e0d\u5339\u914d\u9519\u8bef,\n\u90a3\u53ef\u80fd\u662f ``autocast`` \u6f0f\u6389\u4e86\u4e00\u4e2a op\u3002\n\n\u8bf7\u63d0\u4ea4\u4e00\u4e2a\u5305\u542b\u9519\u8bef\u56de\u6eaf\u7684 issue\u3002\u5728\u8fd0\u884c\u60a8\u7684\u811a\u672c\u4e4b\u524d ``export TORCH_SHOW_CPP_STACKTRACES=1`` \u4ee5\u63d0\u4f9b\u6709\u5173\u54ea\u4e2a\u540e\u7aef op \u5931\u8d25\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}