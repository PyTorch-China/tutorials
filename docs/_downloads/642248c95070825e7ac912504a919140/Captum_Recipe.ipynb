{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u4f7f\u7528 Captum \u8fdb\u884c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum \u53ef\u4ee5\u5e2e\u52a9\u60a8\u4e86\u89e3\u6570\u636e\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u6216\u795e\u7ecf\u5143\u6fc0\u6d3b,\u4ece\u800c\u63ed\u793a\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u3002\n\n\u4f7f\u7528 Captum,\u60a8\u53ef\u4ee5\u7edf\u4e00\u5730\u5e94\u7528\u5e7f\u6cdb\u7684\u6700\u5148\u8fdb\u7684\u7279\u5f81\u5f52\u56e0\u7b97\u6cd5,\u5982 ``Guided GradCam`` \u548c ``Integrated Gradients``\u3002\n\n\u5728\u672c\u6559\u7a0b\u4e2d,\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528 Captum:\n\n- \u5c06\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u9884\u6d4b\u5f52\u56e0\u4e8e\u76f8\u5e94\u7684\u56fe\u50cf\u7279\u5f81\u3002\n- \u53ef\u89c6\u5316\u5f52\u56e0\u7ed3\u679c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u5f00\u59cb\u4e4b\u524d\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u786e\u4fdd\u5728\u60a8\u7684\u6d3b\u8dc3 Python \u73af\u5883\u4e2d\u5b89\u88c5\u4e86 Captum\u3002Captum \u53ef\u4ee5\u5728 GitHub \u4e0a\u83b7\u53d6,\u4e5f\u53ef\u4ee5\u4f5c\u4e3a ``pip`` \u5305\u6216 ``conda`` \u5305\u83b7\u53d6\u3002\n\u6709\u5173\u8be6\u7ec6\u8bf4\u660e,\u8bf7\u67e5\u9605\u5b89\u88c5\u6307\u5357 https://captum.ai/\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5bf9\u4e8e\u6a21\u578b,\u6211\u4eec\u4f7f\u7528 PyTorch \u4e2d\u7684\u5185\u7f6e\u56fe\u50cf\u5206\u7c7b\u5668\u3002Captum \u53ef\u4ee5\u63ed\u793a\u6837\u672c\u56fe\u50cf\u7684\u54ea\u4e9b\u90e8\u5206\u652f\u6301\u4e86\u6a21\u578b\u505a\u51fa\u7684\u67d0\u4e9b\u9884\u6d4b\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\nimport requests\nimport torchvision\nfrom PIL import Image\nfrom torchvision import models, transforms\n\nmodel = torchvision.models.resnet18(\n    weights=models.ResNet18_Weights.IMAGENET1K_V1\n).eval()\n\nresponse = requests.get(\n    \"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\"\n)\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n    ]\n)\n\nnormalize = transforms.Compose(\n    [\n        transforms.ToTensor(),  # \u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u503c\u5728 0 \u5230 1 \u4e4b\u95f4\u7684\u5f20\u91cf\n        transforms.Normalize(  # \u5f52\u4e00\u5316\u4ee5\u9075\u5faa 0 \u5747\u503c\u7684 ImageNet \u50cf\u7d20 RGB \u5206\u5e03\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),\n    ]\n)\ninput_img = normalize(center_crop(img)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u8ba1\u7b97\u5f52\u56e0\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u6a21\u578b\u7684\u524d 3 \u4e2a\u9884\u6d4b\u4e2d,\u7c7b\u522b 208 \u548c 283 \u5206\u522b\u5bf9\u5e94\u4e8e\u72d7\u548c\u732b\u3002\n\n\u8ba9\u6211\u4eec\u4f7f\u7528 Captum \u7684 ``Occlusion`` \u7b97\u6cd5\u5c06\u8fd9\u4e9b\u9884\u6d4b\u5f52\u56e0\u4e8e\u8f93\u5165\u7684\u76f8\u5e94\u90e8\u5206\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from captum.attr import Occlusion\n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)  # \u6b65\u957f\u8d8a\u5c0f,\u5f52\u56e0\u8d8a\u7ec6\u7c92\u5ea6,\u4f46\u901f\u5ea6\u8d8a\u6162\ntarget = (208,)  # ImageNet \u4e2d\u7684\u62c9\u5e03\u62c9\u591a\u7d22\u5f15\nsliding_window_shapes = (3, 45, 45)  # \u9009\u62e9\u8db3\u4ee5\u6539\u53d8\u5bf9\u8c61\u5916\u89c2\u7684\u5927\u5c0f\nbaselines = 0  # \u7528\u4e8e\u906e\u6321\u56fe\u50cf\u7684\u503c\u30020 \u5bf9\u5e94\u7070\u8272\n\nattribution_dog = occlusion.attribute(\n    input_img,\n    strides=strides,\n    target=target,\n    sliding_window_shapes=sliding_window_shapes,\n    baselines=baselines,\n)\n\n\ntarget = (283,)  # ImageNet \u4e2d\u7684\u6ce2\u65af\u732b\u7d22\u5f15\nattribution_cat = occlusion.attribute(\n    input_img,\n    strides=strides,\n    target=target,\n    sliding_window_shapes=sliding_window_shapes,\n    baselines=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u9664\u4e86 ``Occlusion`` \u4e4b\u5916,Captum \u8fd8\u63d0\u4f9b\u4e86\u8bb8\u591a\u7b97\u6cd5,\u5982 ``Integrated Gradients``\u3001``Deconvolution``\u3001\n``GuidedBackprop``\u3001``Guided GradCam``\u3001``DeepLift`` \u548c ``GradientShap``\u3002\u6240\u6709\u8fd9\u4e9b\u7b97\u6cd5\u90fd\u662f ``Attribution`` \u7684\u5b50\u7c7b,\n\u5728\u521d\u59cb\u5316\u65f6\u9700\u8981\u5c06\u60a8\u7684\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u7684 ``forward_func``\u4f20\u5165,\u5e76\u5177\u6709 ``attribute(...)`` \u65b9\u6cd5,\u8be5\u65b9\u6cd5\u4ee5\u7edf\u4e00\u7684\u683c\u5f0f\u8fd4\u56de\u5f52\u56e0\u7ed3\u679c\u3002\n\n\u8ba9\u6211\u4eec\u53ef\u89c6\u5316\u8ba1\u7b97\u51fa\u7684\u56fe\u50cf\u5f52\u56e0\u7ed3\u679c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u53ef\u89c6\u5316\u7ed3\u679c\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum \u7684 ``visualization`` \u5b9e\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5f00\u7bb1\u5373\u7528\u7684\u65b9\u6cd5,\u7528\u4e8e\u53ef\u89c6\u5316\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u7684\u5f52\u56e0\u7ed3\u679c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom captum.attr import visualization as viz\n\n# \u5c06\u8ba1\u7b97\u51fa\u7684\u5f52\u56e0\u5f20\u91cf\u8f6c\u6362\u4e3a\u7c7b\u4f3c\u56fe\u50cf\u7684 numpy \u6570\u7ec4\nattribution_dog = np.transpose(\n    attribution_dog.squeeze().cpu().detach().numpy(), (1, 2, 0)\n)\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"]  # \"positive\"\u3001\"negative\" \u6216 \"all\" \u4ee5\u663e\u793a\u4e24\u8005\n# \u6b63\u5f52\u56e0\u8868\u793a\u8be5\u533a\u57df\u7684\u5b58\u5728\u4f1a\u589e\u52a0\u9884\u6d4b\u5206\u6570\n# \u8d1f\u5f52\u56e0\u8868\u793a\u8be5\u533a\u57df\u7684\u7f3a\u5931\u4f1a\u589e\u52a0\u9884\u6d4b\u5206\u6570\n\n_ = viz.visualize_image_attr_multiple(\n    attribution_dog,\n    np.array(center_crop(img)),\n    vis_types,\n    vis_signs,\n    [\"attribution for dog\", \"image\"],\n    show_colorbar=True,\n)\n\n\nattribution_cat = np.transpose(\n    attribution_cat.squeeze().cpu().detach().numpy(), (1, 2, 0)\n)\n\n_ = viz.visualize_image_attr_multiple(\n    attribution_cat,\n    np.array(center_crop(img)),\n    [\"heat_map\", \"original_image\"],\n    [\"all\", \"all\"],  # \u6b63/\u8d1f\u5f52\u56e0\u6216\u5168\u90e8\n    [\"attribution for cat\", \"image\"],\n    show_colorbar=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5982\u679c\u60a8\u7684\u6570\u636e\u662f\u6587\u672c,``visualization.visualize_text()`` \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u7528\u89c6\u56fe,\u7528\u4e8e\u63a2\u7d22\u8f93\u5165\u6587\u672c\u7684\u5f52\u56e0\u3002\n\u66f4\u591a\u4fe1\u606f\u8bf7\u8bbf\u95ee http://captum.ai/tutorials/IMDB_TorchText_Interpret\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u6700\u540e\u6ce8\u610f\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum \u53ef\u4ee5\u5904\u7406 PyTorch \u4e2d\u5305\u62ec\u89c6\u89c9\u3001\u6587\u672c\u7b49\u5404\u79cd\u6a21\u6001\u7684\u5927\u591a\u6570\u6a21\u578b\u7c7b\u578b\u3002\u4f7f\u7528 Captum \u60a8\u53ef\u4ee5:\n* \u5c06\u7279\u5b9a\u8f93\u51fa\u5f52\u56e0\u4e8e\u6a21\u578b\u8f93\u5165,\u5982\u4e0a\u6240\u793a\u3002\n* \u5c06\u7279\u5b9a\u8f93\u51fa\u5f52\u56e0\u4e8e\u9690\u85cf\u5c42\u795e\u7ecf\u5143(\u53c2\u89c1 Captum API \u53c2\u8003)\u3002\n* \u5c06\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u54cd\u5e94\u5f52\u56e0\u4e8e\u6a21\u578b\u8f93\u5165(\u53c2\u89c1 Captum API \u53c2\u8003)\u3002\n\n\u6709\u5173\u652f\u6301\u65b9\u6cd5\u7684\u5b8c\u6574 API \u548c\u6559\u7a0b\u5217\u8868,\u8bf7\u67e5\u9605\u6211\u4eec\u7684\u7f51\u7ad9 http://captum.ai\n\nGilbert Tanner \u7684\u53e6\u4e00\u7bc7\u6709\u7528\u6587\u7ae0:\nhttps://gilberttanner.com/blog/interpreting-pytorch-models-with-captum\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}