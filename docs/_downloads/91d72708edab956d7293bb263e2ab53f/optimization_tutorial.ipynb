{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n[Learn the Basics](intro.html) ||\n[Quickstart](quickstart_tutorial.html) ||\n[Tensors](tensorqs_tutorial.html) ||\n[Datasets & DataLoaders](data_tutorial.html) ||\n[Transforms](transforms_tutorial.html) ||\n[Build Model](buildmodel_tutorial.html) ||\n[Autograd](autogradqs_tutorial.html) ||\n**Optimization** ||\n[Save & Load Model](saveloadrun_tutorial.html)\n\n# \u4f18\u5316\u6a21\u578b\u53c2\u6570\n\n\u73b0\u5728\u6211\u4eec\u6709\u4e86\u6a21\u578b\u548c\u6570\u636e\uff0c\u662f\u65f6\u5019\u901a\u8fc7\u5728\u6570\u636e\u4e0a\u4f18\u5316\u6a21\u578b\u53c2\u6570\u6765\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6211\u4eec\u7684\u6a21\u578b\u4e86\u3002\u8bad\u7ec3\u6a21\u578b\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff1b\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u6a21\u578b\u4f1a\u5bf9\u8f93\u51fa\u8fdb\u884c\u731c\u6d4b\uff0c\n\u8ba1\u7b97\u5176\u731c\u6d4b\u7684\u8bef\u5dee\uff08*\u635f\u5931-loss*\uff09\uff0c\u6536\u96c6\u8bef\u5dee\u76f8\u5bf9\u4e8e\u5176\u53c2\u6570\u7684\u5bfc\u6570\uff08\u5982\u6211\u4eec\u5728[\u524d\u4e00\u8282](autograd_tutorial.html)\u4e2d\u6240\u89c1\uff09\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5**\u4f18\u5316**\u8fd9\u4e9b\u53c2\u6570\u3002\n\u6709\u5173\u6b64\u8fc7\u7a0b\u7684\u66f4\u8be6\u7ec6\u8bb2\u89e3\uff0c\u8bf7\u67e5\u770b 3Blue1Brown \u7684\u8fd9\u4e2a\u89c6\u9891[\u53cd\u5411\u4f20\u64ad](https://www.youtube.com/watch?v=tIeHLnjs5U8)_\u3002\n\n## \u524d\u7f6e\u4ee3\u7801\n\u6211\u4eec\u52a0\u8f7d\u524d\u51e0\u8282\u4e2d\u7684[\u6570\u636e\u96c6\u548c\u6570\u636e\u52a0\u8f7d\u5668](data_tutorial.html)\u548c[\u6784\u5efa\u6a21\u578b](buildmodel_tutorial.html)\u7684\u4ee3\u7801\u3002\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u8d85\u53c2\u6570\n\n\u8d85\u53c2\u6570\u662f\u53ef\u8c03\u53c2\u6570\uff0c\u5b83\u4eec\u53ef\u4ee5\u8ba9\u60a8\u63a7\u5236\u6a21\u578b\u7684\u4f18\u5316\u8fc7\u7a0b\u3002\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u503c\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u6536\u655b\u901f\u5ea6\n\uff08[\u9605\u8bfb\u66f4\u591a](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)_ \u5173\u4e8e\u8d85\u53c2\u6570\u8c03\u6574\u7684\u5185\u5bb9\uff09\u3002\n\n\u6211\u4eec\u4e3a\u8bad\u7ec3\u5b9a\u4e49\u4ee5\u4e0b\u8d85\u53c2\u6570\uff1a\n - **Epoch\u6570\u91cf** - \u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6\u7684\u6b21\u6570\n - **\u6279\u91cf\u5927\u5c0f** - \u5728\u66f4\u65b0\u53c2\u6570\u4e4b\u524d\uff0c\u901a\u8fc7\u7f51\u7edc\u4f20\u64ad\u7684\u6570\u636e\u6837\u672c\u6570\u91cf\n - **\u5b66\u4e60\u7387** - \u5728\u6bcf\u4e2a\u6279\u6b21/epoch\u4e2d\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u5e45\u5ea6\u3002\u8f83\u5c0f\u7684\u503c\u4f1a\u5bfc\u81f4\u5b66\u4e60\u901f\u5ea6\u7f13\u6162\uff0c\u800c\u8f83\u5927\u7684\u503c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 5\nbatch_size = 64\nlearning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u4f18\u5316\u5faa\u73af\n\n\u4e00\u65e6\u8bbe\u7f6e\u597d\u8d85\u53c2\u6570\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u7528\u4f18\u5316\u5faa\u73af\u6765\u8bad\u7ec3\u548c\u4f18\u5316\u6211\u4eec\u7684\u6a21\u578b\u3002\u4f18\u5316\u5faa\u73af\u7684\u6bcf\u6b21\u8fed\u4ee3\u79f0\u4e3a\u4e00\u4e2a**epoch**\u3002\n\n\u6bcf\u4e2aepoch\u7531\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\u7ec4\u6210\uff1a\n - **\u8bad\u7ec3\u5faa\u73af** - \u8fed\u4ee3\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u5c1d\u8bd5\u6536\u655b\u5230\u6700\u4f73\u53c2\u6570\u3002\n - **\u9a8c\u8bc1/\u6d4b\u8bd5\u5faa\u73af** - \u8fed\u4ee3\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u68c0\u67e5\u6a21\u578b\u6027\u80fd\u662f\u5426\u6709\u63d0\u9ad8\u3002\n\n\u8ba9\u6211\u4eec\u7b80\u8981\u4e86\u89e3\u8bad\u7ec3\u5faa\u73af\u4e2d\u4f7f\u7528\u7684\u4e00\u4e9b\u6982\u5ff5\u3002\u8df3\u5230\u524d\u9762\u67e5\u770b\u4f18\u5316\u5faa\u73af\u7684 `full-impl-label` \u3002\n\n### \u635f\u5931\u51fd\u6570(Loss Function)\n\n\u5f53\u9762\u5bf9\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u6211\u4eec\u672a\u8bad\u7ec3\u7684\u7f51\u7edc\u53ef\u80fd\u4e0d\u4f1a\u7ed9\u51fa\u6b63\u786e\u7684\u7b54\u6848\u3002**\u635f\u5931\u51fd\u6570(Loss Function)** \u8861\u91cf\u83b7\u5f97\u7684\u7ed3\u679c\u4e0e\u76ee\u6807\u503c\u7684\u5dee\u5f02\u7a0b\u5ea6\uff0c\n\u8fd9\u662f\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e0c\u671b\u6700\u5c0f\u5316\u7684\u3002\u8981\u8ba1\u7b97\u635f\u5931\uff0c\u6211\u4eec\u4f7f\u7528\u7ed9\u5b9a\u6570\u636e\u6837\u672c\u7684\u8f93\u5165\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5c06\u5176\u4e0e\u771f\u5b9e\u7684\u6570\u636e\u6807\u7b7e\u503c\u8fdb\u884c\u6bd4\u8f83\u3002\n\n\u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570\u5305\u62ec\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\u7684[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\uff08\u5747\u65b9\u8bef\u5dee\uff09\uff0c\n\u4ee5\u53ca\u7528\u4e8e\u5206\u7c7b\u7684[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)\uff08\u8d1f\u5bf9\u6570\u4f3c\u7136\uff09\u3002\n[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\u7ed3\u5408\u4e86``nn.LogSoftmax``\u548c``nn.NLLLoss``\u3002\n\n\u6211\u4eec\u5c06\u6a21\u578b\u7684\u8f93\u51falogits\u4f20\u9012\u7ed9``nn.CrossEntropyLoss``\uff0c\u5b83\u5c06\u6807\u51c6\u5316logits\u5e76\u8ba1\u7b97\u9884\u6d4b\u8bef\u5dee\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\nloss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u4f18\u5316\u5668\n\n\u4f18\u5316\u662f\u8c03\u6574\u6a21\u578b\u53c2\u6570\u4ee5\u51cf\u5c11\u6bcf\u6b21\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u7684\u6a21\u578b\u8bef\u5dee\u7684\u8fc7\u7a0b\u3002**\u4f18\u5316\u7b97\u6cd5**\u5b9a\u4e49\u4e86\u8fd9\u4e2a\u8fc7\u7a0b\u5982\u4f55\u8fdb\u884c\uff08\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5\uff09\u3002\n\u6240\u6709\u4f18\u5316\u903b\u8f91\u90fd\u5c01\u88c5\u5728``optimizer``\u5bf9\u8c61\u4e2d\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528SGD\u4f18\u5316\u5668\uff1b\u6b64\u5916\uff0cPyTorch\u4e2d\u8fd8\u6709\u8bb8\u591a[\u4e0d\u540c\u7684\u4f18\u5316\u5668](https://pytorch.org/docs/stable/optim.html)\uff0c\n\u5982ADAM\u548cRMSProp\uff0c\u5b83\u4eec\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u548c\u6570\u636e\u6548\u679c\u66f4\u597d\u3002\n\n\u6211\u4eec\u901a\u8fc7\u6ce8\u518c\u9700\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u53c2\u6570\u5e76\u4f20\u5165\u5b66\u4e60\u7387\u8d85\u53c2\u6570\u6765\u521d\u59cb\u5316\u4f18\u5316\u5668\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u4f18\u5316\u5206\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\n * \u8c03\u7528``optimizer.zero_grad()``\u6765\u91cd\u7f6e\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u68af\u5ea6\u4f1a\u7d2f\u52a0\uff1b\u4e3a\u9632\u6b62\u91cd\u590d\u8ba1\u7b97\uff0c\u6211\u4eec\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u663e\u5f0f\u5c06\u5176\u5f52\u96f6\u3002\n * \u8c03\u7528``loss.backward()``\u53cd\u5411\u4f20\u64ad\u9884\u6d4b\u635f\u5931\u3002PyTorch\u4f1a\u5c06\u635f\u5931\u76f8\u5bf9\u4e8e\u6bcf\u4e2a\u53c2\u6570\u7684\u68af\u5ea6\u5b58\u50a8\u4e0b\u6765\u3002\n * \u4e00\u65e6\u6211\u4eec\u6709\u4e86\u68af\u5ea6\uff0c\u5c31\u8c03\u7528``optimizer.step()``\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4e2d\u6536\u96c6\u7684\u68af\u5ea6\u6765\u8c03\u6574\u53c2\u6570\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## \u5b8c\u6574\u5b9e\u73b0\n\u6211\u4eec\u5b9a\u4e49\u4e86``train_loop``\u6765\u5faa\u73af\u6267\u884c\u4f18\u5316\u4ee3\u7801\uff0c\u5e76\u5b9a\u4e49\u4e86``test_loop``\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6211\u4eec\u521d\u59cb\u5316\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9``train_loop``\u548c``test_loop``\u3002\n\u60a8\u53ef\u4ee5\u5c1d\u8bd5\u589e\u52a0epoch\u7684\u6570\u91cf\u4ee5\u89c2\u5bdf\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u5ef6\u4f38\u9605\u8bfb\n- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n- [torch.optim](https://pytorch.org/docs/stable/optim.html)\n- [Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}