{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u4f7f\u7528 ``torch.compile`` \u548c\u7528\u6237\u81ea\u5b9a\u4e49\u7684 Triton \u5185\u6838\n**\u4f5c\u8005:** [Oguz Ulgen](https://github.com/oulgen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7528\u6237\u81ea\u5b9a\u4e49\u7684 Triton \u5185\u6838\u53ef\u7528\u4e8e\u4f18\u5316\u6a21\u578b\u8ba1\u7b97\u7684\u7279\u5b9a\u90e8\u5206\u3002\u8fd9\u4e9b\u5185\u6838\u662f\u7528 Triton \u8bed\u8a00\u7f16\u5199\u7684,\n\u65e8\u5728\u66f4\u5bb9\u6613\u5b9e\u73b0\u786c\u4ef6\u7684\u5cf0\u503c\u6027\u80fd\u3002\u901a\u8fc7\u5728 ``torch.compile`` \u4e2d\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684 Triton \u5185\u6838,\n\u60a8\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u4f18\u5316\u8fc7\u7684\u8ba1\u7b97\u96c6\u6210\u5230 PyTorch \u6a21\u578b\u4e2d,\u4ece\u800c\u53ef\u80fd\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\n\n\u672c\u6559\u7a0b\u6f14\u793a\u4e86\u5982\u4f55\u5728 ``torch.compile`` \u4e2d\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684 Triton \u5185\u6838\u3002\n\n## \u5148\u51b3\u6761\u4ef6\n\n\u5728\u5f00\u59cb\u672c\u6559\u7a0b\u4e4b\u524d,\u8bf7\u786e\u4fdd\u60a8\u5177\u5907\u4ee5\u4e0b\u6761\u4ef6:\n\n* \u5bf9 ``torch.compile`` \u548c Triton \u6709\u57fa\u672c\u7684\u4e86\u89e3\u3002\u53c2\u89c1:\n\n  * [torch.compiler API \u6587\u6863](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler)_\n  * [torch.compile \u4ecb\u7ecd](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)_\n  * [Triton \u8bed\u8a00\u6587\u6863](https://triton-lang.org/main/index.html)_\n\n* PyTorch 2.3 \u6216\u66f4\u9ad8\u7248\u672c\n* \u652f\u6301 Triton \u7684 GPU\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils._triton import has_triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u57fa\u672c\u7528\u6cd5\n\n\u5728\u6b64\u793a\u4f8b\u4e2d,\u6211\u4eec\u5c06\u4f7f\u7528\u6765\u81ea Triton \u6587\u6863\u7684\u4e00\u4e2a\u7b80\u5355\u5411\u91cf\u52a0\u6cd5\u5185\u6838\u4e0e ``torch.compile``\u3002\n\u53c2\u8003 [Triton \u6587\u6863](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html)_\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"\u7531\u4e8e\u6b64\u8bbe\u5907\u4e0d\u652f\u6301 triton,\u56e0\u6b64\u8df3\u8fc7\u3002\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.jit\n    def add_kernel(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"\u5411\u91cf\u52a0\u6cd5\\nX:\\t{x}\\nY:\\t{y}\\n\u7ed3\u679c\u4e3a\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u9ad8\u7ea7\u7528\u6cd5\n\nTriton \u7684\u81ea\u52a8\u8c03\u4f18\u529f\u80fd\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177,\u53ef\u81ea\u52a8\u4f18\u5316 Triton \u5185\u6838\u7684\u914d\u7f6e\u53c2\u6570\u3002\n\u5b83\u63a2\u7d22\u4e00\u7cfb\u5217\u53ef\u80fd\u7684\u914d\u7f6e,\u5e76\u9009\u62e9\u4e3a\u60a8\u7684\u7279\u5b9a\u7528\u4f8b\u63d0\u4f9b\u6700\u4f73\u6027\u80fd\u7684\u914d\u7f6e\u3002\n\n\u4e0e ``torch.compile`` \u4e00\u8d77\u4f7f\u7528\u65f6, ``triton.autotune`` \u53ef\u4ee5\u5e2e\u52a9\u786e\u4fdd\u60a8\u7684 PyTorch \u6a21\u578b\u4ee5\u6700\u9ad8\u6548\u7684\u65b9\u5f0f\u8fd0\u884c\u3002\n\u4e0b\u9762\u662f\u4f7f\u7528 ``torch.compile`` \u548c ``triton.autotune`` \u7684\u793a\u4f8b\u3002\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.compile`` \u4ec5\u652f\u6301 ``triton.autotune`` \u7684\u914d\u7f6e\u548c\u5173\u952e\u53c2\u6570\u3002</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"\u7531\u4e8e\u6b64\u8bbe\u5907\u4e0d\u652f\u6301 triton,\u56e0\u6b64\u8df3\u8fc7\u3002\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.autotune(\n        configs=[\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n        ],\n        key=[],\n    )\n    @triton.jit\n    def add_kernel_autotuned(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel_autotuned[grid](x, y, output, n_elements)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"\u5411\u91cf\u52a0\u6cd5\\nX:\\t{x}\\nY:\\t{y}\\n\u7ed3\u679c\u4e3a\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u53ef\u7ec4\u5408\u6027\u548c\u9650\u5236\n\n\u4ece PyTorch 2.3 \u5f00\u59cb, ``torch.compile`` \u4e2d\u5bf9\u7528\u6237\u81ea\u5b9a\u4e49 Triton \u5185\u6838\u7684\u652f\u6301\u5305\u62ec\u52a8\u6001\u5f62\u72b6\u3001\n``torch.autograd.Function``\u3001JIT inductor \u548c AOT inductor\u3002\n\u60a8\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u529f\u80fd\u7ec4\u5408\u5728\u4e00\u8d77\u6784\u5efa\u590d\u6742\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u3002\n\n\u4f46\u662f,\u4e5f\u9700\u8981\u6ce8\u610f\u4e00\u4e9b\u9650\u5236:\n\n* **Tensor \u5b50\u7c7b:** \u76ee\u524d\u4e0d\u652f\u6301\u5f20\u91cf\u5b50\u7c7b\u548c\u5176\u4ed6\u9ad8\u7ea7\u529f\u80fd\u3002\n* **Triton \u529f\u80fd:** \u867d\u7136 ``triton.heuristics`` \u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u6216\u5728 ``triton.autotune`` \u4e4b\u524d\u4f7f\u7528,\n  \u4f46\u4e0d\u80fd\u5728 ``triton.autotune`` \u4e4b\u540e\u4f7f\u7528\u3002\u8fd9\u610f\u5473\u7740\u5982\u679c\u8981\u4e00\u8d77\u4f7f\u7528 ``triton.heuristics`` \u548c ``triton.autotune``,\n  \u5219\u5fc5\u987b\u5148\u4f7f\u7528 ``triton.heuristics``\u3002\n\n## \u7ed3\u8bba\n\u5728\u672c\u6559\u7a0b\u4e2d,\u6211\u4eec\u63a2\u8ba8\u4e86\u5982\u4f55\u5728 ``torch.compile`` \u4e2d\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684 Triton \u5185\u6838\u3002\n\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u4f7f\u7528\u7b80\u5355\u5411\u91cf\u52a0\u6cd5\u5185\u6838\u7684\u57fa\u672c\u7528\u6cd5,\u4ee5\u53ca\u6d89\u53ca Triton \u81ea\u52a8\u8c03\u4f18\u529f\u80fd\u7684\u9ad8\u7ea7\u7528\u6cd5\u3002\n\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u7528\u6237\u81ea\u5b9a\u4e49 Triton \u5185\u6838\u4e0e\u5176\u4ed6 PyTorch \u529f\u80fd\u7684\u53ef\u7ec4\u5408\u6027,\u5e76\u5f3a\u8c03\u4e86\u4e00\u4e9b\u5f53\u524d\u7684\u9650\u5236\u3002\n\n## \u53e6\u8bf7\u53c2\u9605\n\n* [\u7f16\u8bd1\u4f18\u5316\u5668](https://pytorch.org/tutorials/recipes/compiling_optimizer.html)_\n* [\u4f7f\u7528\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u5b9e\u73b0\u9ad8\u6027\u80fd Transformer](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}