{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using User-Defined Triton Kernels with ``torch.compile``\n**Author:** [Oguz Ulgen](https://github.com/oulgen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "User-defined Triton kernels can be used to optimize specific parts of your\nmodel's computation. These kernels are written in Triton's language, which is designed\nto make it easier to achieve peak hardware performance. By using user-defined Triton\nkernels with ``torch.compile``, you can integrate these optimized computations into\nyour PyTorch model, potentially achieving significant performance improvements.\n\nThis recipes demonstrates how you can use user-defined Triton kernels with ``torch.compile``.\n\n## Prerequisites\n\nBefore starting this recipe, make sure that you have the following:\n\n* Basic understanding of ``torch.compile`` and Triton. See:\n\n  * [torch.compiler API documentation](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler)_\n  * [Introduction to torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)_\n  * [Triton language documentation](https://triton-lang.org/main/index.html)_\n\n* PyTorch 2.3 or later\n* A GPU that supports Triton\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils._triton import has_triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Usage\n\nIn this example, we will use a simple vector addition kernel from the Triton documentation\nwith ``torch.compile``.\nFor reference, see [Triton documentation](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.jit\n    def add_kernel(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n\nTriton's autotune feature is a powerful tool that automatically optimizes the configuration\nparameters of your Triton kernels. It explores a range of possible configurations and\nselects the one that delivers the best performance for your specific use case.\n\nWhen used with ``torch.compile``, ``triton.autotune`` can help ensure that your PyTorch\nmodel is running as efficiently as possible. Here is an example of using ``torch.compile``\nand ``triton.autotune``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.compile`` only supports configs and key arguments to ``triton.autotune``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.autotune(\n        configs=[\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n        ],\n        key=[],\n    )\n    @triton.jit\n    def add_kernel_autotuned(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel_autotuned[grid](x, y, output, n_elements)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Composibility and Limitations\n\nAs of PyTorch 2.3, the support for user-defined Triton kernels in ``torch.compile``\nincludes dynamic shapes, ``torch.autograd.Function``, JIT inductor, and AOT inductor.\nYou can use these features together to build complex, high-performance models.\n\nHowever, there are certain limitations to be aware of:\n\n* **Tensor Subclasses:** Currently, there is no support for\n  tensor subclasses and other advanced features.\n* **Triton Features:** While ``triton.heuristics`` can be used either standalone or\n  before ``triton.autotune``, it cannot be used after ```triton.autotune``. This\n  implies that if ``triton.heuristics`` and ``triton.autotune`` are to be used\n  together, ``triton.heuristics`` must be used first.\n\n## Conclusion\nIn this recipe, we explored how to utilize user-defined Triton kernels\nwith ``torch.compile``. We delved into the basic usage of a simple\nvector addition kernel and advanced usage involving Triton's autotune\nfeature. We also discussed the composability of user-defined Triton\nkernels with other PyTorch features and highlighted some current limitations.\n\n## See Also\n\n* [Compiling the Optimizers](https://pytorch.org/tutorials/recipes/compiling_optimizer.html)_\n* [Implementing High-Performance Transformers with Scaled Dot Product Attention](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}